{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"BBox\" class=\"alert alert-info\" style=\"font-family:courier;color:black;justify-content:left;\">\n",
    "<h1> Distance Operations </h1>\n",
    "NumPy itself doesn‚Äôt have built-in distance functions, but it supports vectorized computations that make distance calculations efficient. Commonly, distances can be computed using functions available in scipy.spatial.distance or by leveraging NumPy operations. Below are key distance-related functions and methods in NumPy:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Math, Latex\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"BBox\" class=\"alert alert-info\" style=\"font-family:courier;color:black;justify-content:left;\">\n",
    "<h3> Euclidean Distance</h3>\n",
    "<strong>What It Is:</strong><br>\n",
    "Euclidean distance is the straight-line distance between two points in Euclidean space. It is computed using the formula given below.\n",
    "<br><br>\n",
    "<strong>How It‚Äôs Used:</strong> <br>\n",
    "point1 = np.array([1, 2]) <br>\n",
    "point2 = np.array([4, 6]) <br>\n",
    "distance = np.linalg.norm(point1 - point2)<br>\n",
    "<br>\n",
    "<strong>Why it is important:</strong> <br>\n",
    "Euclidean distance is widely used in machine learning for similarity measurement in clustering algorithms (like K-Means) and in nearest-neighbor methods (like KNN).\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle d\\left( p,q\\right)   = \\sqrt {\\sum _{i=1}^{n}  \\left( q_{i}-p_{i}\\right)^2 }$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Math(r'd\\left( p,q\\right)   = \\sqrt {\\sum _{i=1}^{n}  \\left( q_{i}-p_{i}\\right)^2 }'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "point1 = np.array([1, 2])\n",
    "point2 = np.array([4, 6])\n",
    "distance = np.linalg.norm(point1 - point2)\n",
    "print(distance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"BBox\" class=\"alert alert-info\" style=\"font-family:courier;color:black;justify-content:left;\">\n",
    "<h3> Manhattan Distance</h3>\n",
    "<strong>What It Is:</strong><br>\n",
    "Manhattan distance, also known as L1 distance, measures the sum of absolute differences between the coordinates of two points.\n",
    "<br><br>\n",
    "<strong>How It‚Äôs Used:</strong> <br>\n",
    "point1 = np.array([1, 2]) <br>\n",
    "point2 = np.array([4, 6]) <br>\n",
    "distance = np.sum(np.abs(point1 - point2))<br>\n",
    "<br>\n",
    "<strong>Why it is important:</strong> <br>\n",
    "Manhattan distance is useful when dealing with grid-like path calculations and is often used in ML for features where high dimensions and sparse data are involved.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle d_{\\text{Manhattan}} = \\sum_{i=1}^n |x_i - y_i|$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Math(r'd_{\\text{Manhattan}} = \\sum_{i=1}^n |x_i - y_i|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "point1 = np.array([1, 2])\n",
    "point2 = np.array([4, 6])\n",
    "distance = np.sum(np.abs(point1 - point2))\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"BBox\" class=\"alert alert-info\" style=\"font-family:courier;color:black;justify-content:left;\">\n",
    "<h3> Cosine Similarity</h3>\n",
    "<strong>What It Is:</strong><br>\n",
    "Cosine similarity measures the cosine of the angle between two vectors, representing how similar the two vectors are regardless of their magnitudes.\n",
    "<br><br>\n",
    "<strong>How It‚Äôs Used:</strong> <br>\n",
    "point1 = np.array([1, 2]) <br>\n",
    "point2 = np.array([4, 6]) <br>\n",
    "cosine_similarity = np.dot(point1, point2) / (np.linalg.norm(point1) * np.linalg.norm(point2))\n",
    "<br><br>\n",
    "<strong>Why it is important:</strong> <br>\n",
    "Cosine similarity is crucial in NLP and recommendation systems, where the direction rather than the magnitude of vectors is important, like in comparing text documents or user preferences.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\text{cosine\\_similarity} = \\cos(\\theta) = \\frac{\\vec{A} \\cdot \\vec{B}}{\\|\\vec{A}\\| \\|\\vec{B}\\|}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Math(r'\\text{cosine\\_similarity} = \\cos(\\theta) = \\frac{\\vec{A} \\cdot \\vec{B}}{\\|\\vec{A}\\| \\|\\vec{B}\\|}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2]\n",
      "[4 6]\n",
      "Dot Product:  16\n",
      "Norm of Point 1:  2.23606797749979\n",
      "Norm of Point 2:  7.211102550927978\n",
      "Cosine Similarity:  0.9922778767136677\n"
     ]
    }
   ],
   "source": [
    "point1 = np.array([1, 2])\n",
    "point2 = np.array([4, 6])\n",
    "print(point1)\n",
    "print(point2)\n",
    "dot_product = np.dot(point1, point2)\n",
    "print(\"Dot Product: \",dot_product)\n",
    "norm_point1 = np.linalg.norm(point1)\n",
    "print(\"Norm of Point 1: \", norm_point1)\n",
    "norm_point2 = np.linalg.norm(point2)\n",
    "print(\"Norm of Point 2: \", norm_point2)\n",
    "cosine_similarity = dot_product / ( norm_point1 * norm_point2)\n",
    "print(\"Cosine Similarity: \",cosine_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"BBox\" class=\"alert alert-info\" style=\"font-family:courier;color:black;justify-content:left;\">\n",
    "<h3> Minkowski Distance</h3>\n",
    "<strong>What It Is:</strong><br>\n",
    "Minkowski distance is a generalization of Euclidean and Manhattan distances, parameterized by a value \\( p \\), where \\( p = 1 \\) gives Manhattan distance and \\( p = 2 \\) gives Euclidean distance.\n",
    "<br><br>\n",
    "<strong>How It‚Äôs Used:</strong> <br>\n",
    "p = 3 <br>\n",
    "point1 = np.array([1, 2]) <br>\n",
    "point2 = np.array([4, 6]) <br>\n",
    "distance = np.sum(np.abs(point1 - point2) ** p) ** (1/p)\n",
    "<br><br>\n",
    "<strong>Why it is important:</strong> <br>\n",
    "Minkowski distance offers flexibility for different distance calculations in machine learning, allowing adjustments for varying data characteristics.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle d_{\\text{Minkowski}} = \\left( \\sum_{i=1}^n |x_i - y_i|^p \\right)^{\\frac{1}{p}}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Math(r'd_{\\text{Minkowski}} = \\left( \\sum_{i=1}^n |x_i - y_i|^p \\right)^{\\frac{1}{p}}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.497941445275415\n"
     ]
    }
   ],
   "source": [
    "p = 3\n",
    "point1 = np.array([1, 2])\n",
    "point2 = np.array([4, 6])\n",
    "distance = np.sum(np.abs(point1 - point2) ** p) ** (1/p)\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"BBox\" class=\"alert alert-info\" style=\"font-family:courier;color:black;justify-content:left;\">\n",
    "<h3> Hamming  Distance</h3>\n",
    "<strong>What It Is:</strong><br>\n",
    "Hamming distance calculates the proportion of differing elements between two binary or categorical vectors.\n",
    "<br><br>\n",
    "<strong>How It‚Äôs Used:</strong> <br>\n",
    "vector1 = np.array([1, 0, 1, 1]) <br>\n",
    "vector2 = np.array([1, 1, 0, 1]) <br>\n",
    "hamming_distance = np.mean(vector1 != vector2)\n",
    "<br><br>\n",
    "<strong>Why it is important:</strong> <br>\n",
    "Hamming distance is widely used in error detection, information retrieval, and in ML for comparing binary strings or categorical data.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle d_{\\text{Hamming}} = \\sum_{i=1}^n \\delta(x_i, y_i)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle where$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \n",
       "                \\delta(x_i, y_i) = \n",
       "                \\begin{cases} \n",
       "                    0 & \\text{if } x_i = y_i \\\\\n",
       "                    1 & \\text{if } x_i \\neq y_i \n",
       "                \\end{cases}\n",
       "             $"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Math(r'd_{\\text{Hamming}} = \\sum_{i=1}^n \\delta(x_i, y_i)'))\n",
    "display(Math(r'where'))\n",
    "display(Math(r'''\n",
    "                \\delta(x_i, y_i) = \n",
    "                \\begin{cases} \n",
    "                    0 & \\text{if } x_i = y_i \\\\\n",
    "                    1 & \\text{if } x_i \\neq y_i \n",
    "                \\end{cases}\n",
    "             '''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "vector1 = np.array([1, 0, 1, 1])\n",
    "vector2 = np.array([1, 1, 0, 1])\n",
    "hamming_distance = np.mean(vector1 != vector2)\n",
    "print(hamming_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"BBox\" class=\"alert alert-info\" style=\"font-family:courier;color:black;justify-content:left;\">\n",
    "<h3> Jaccard Similarity</h3>\n",
    "<strong>What It Is:</strong><br>\n",
    "Jaccard similarity measures the similarity between two binary arrays, calculated as the ratio of the intersection to the union of the arrays.\n",
    "<br><br>\n",
    "<strong>How It‚Äôs Used:</strong> <br>\n",
    "vector1 = np.array([1, 0, 1, 1]) <br>\n",
    "vector2 = np.array([1, 1, 0, 1]) <br>\n",
    "intersection = np.logical_and(vector1, vector2).sum() <br>\n",
    "union = np.logical_or(vector1, vector2).sum() <br>\n",
    "jaccard_similarity = intersection / union\n",
    "<br><br>\n",
    "<strong>Why it is important:</strong> <br>\n",
    "Jaccard similarity is useful in document similarity, clustering, and other ML tasks involving categorical or binary data, as it focuses on the degree of overlap.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\text{Jaccard\\_similarity} = \\frac{|A \\cap B|}{|A \\cup B|}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Math(r'\\text{Jaccard\\_similarity} = \\frac{|A \\cap B|}{|A \\cup B|}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "vector1 = np.array([1, 0, 1, 1])\n",
    "vector2 = np.array([1, 1, 0, 1])\n",
    "intersection = np.logical_and(vector1, vector2).sum()\n",
    "union = np.logical_or(vector1, vector2).sum()\n",
    "jaccard_similarity = intersection / union\n",
    "\n",
    "print(jaccard_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"BBox\" class=\"alert alert-info\" style=\"font-family:courier;color:black;justify-content:left;\">\n",
    "<h3> Mahalanobis Distance</h3>\n",
    "<strong>What It Is:</strong><br>\n",
    "Mahalanobis distance is a measure of distance between two points in a multivariate space. Unlike Euclidean distance, which assumes all features have the same scale and no correlation, Mahalanobis distance takes into account the correlations between features by using the covariance matrix of the data. It is particularly useful in identifying outliers and measuring distances in high-dimensional data with dependencies.\n",
    "<br><br>\n",
    "<strong>How It‚Äôs Used:</strong> <br>\n",
    "To compute Mahalanobis distance in Python with NumPy, we need: <br>\n",
    "1. The data vectors ùë• and ùë¶. <br>\n",
    "2. The covariance matrix Œ£ of the dataset. <br>\n",
    "3. The inverse of the covariance matrix Œ£. <br>\n",
    "<br><br>\n",
    "<strong>Why it is important:</strong> <br>\n",
    "<ul>\n",
    "<li><strong>Handles Correlated Data:</strong> Mahalanobis distance considers the correlation between features, making it suitable for multivariate data where variables are dependent.</li>\n",
    "<li><strong>Outlier Detection: </strong>This distance metric is effective for identifying outliers in data by measuring how far a point deviates from a distribution.</li>\n",
    "<li><strong>Invariant to Scale:</strong> Mahalanobis distance is scale-invariant, meaning it accounts for differences in units across features, which is helpful when working with features of varying scales.</li>\n",
    "<ul>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle d_{\\text{Mahalanobis}} = \\sqrt{(\\vec{x} - \\vec{y})^T \\Sigma^{-1} (\\vec{x} - \\vec{y})}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Math(r'd_{\\text{Mahalanobis}} = \\sqrt{(\\vec{x} - \\vec{y})^T \\Sigma^{-1} (\\vec{x} - \\vec{y})}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.455357119316824\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "data = np.array([\n",
    "    [4, 2, 0],\n",
    "    [4, 5, 6],\n",
    "    [10, 3, 2],\n",
    "    [3, 7, 4],\n",
    "    [6, 5, 5],\n",
    "])\n",
    "\n",
    "# Vectors between which we calculate the Mahalanobis distance\n",
    "x = np.array([4, 2, 0])\n",
    "y = np.array([6, 5, 5])\n",
    "\n",
    "# Calculate the covariance matrix of the dataset\n",
    "cov_matrix = np.cov(data, rowvar=False)\n",
    "# Invert the covariance matrix\n",
    "cov_matrix_inv = np.linalg.inv(cov_matrix)\n",
    "# Calculate the difference vector\n",
    "delta = x - y\n",
    "# Calculate the Mahalanobis distance\n",
    "distance = np.sqrt(delta.T @ cov_matrix_inv @ delta)  # @ is for matrix multiplication\n",
    "\n",
    "print(distance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_2.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
